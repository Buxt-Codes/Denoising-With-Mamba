{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b196e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from training_utils import classification_training, contrastive_training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from models import MambaPooled, MambaCLS, CrossAttentionTransformer, NomicEmbedder\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327643df",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.7\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "vectors_df = pd.read_pickle(r\"C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\data\\processed\\vectorised_locations.pkl\")\n",
    "vectors_map = dict(zip(vectors_df[\"gmap_id\"], vectors_df[\"vector\"]))\n",
    "\n",
    "positive_reviews = pd.read_csv(r\"C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\data\\processed\\positive_reviews.csv\")       # label 1\n",
    "ads_reviews = pd.read_csv(r\"C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\data\\processed\\ads_reviews.csv\")                 # label 0\n",
    "indirect_reviews = pd.read_csv(r\"C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\data\\processed\\indirect_reviews.csv\")       # label 0\n",
    "irelevant_reviews = pd.read_csv(r\"C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\data\\processed\\irelevant_reviews.csv\")     # label 0\n",
    "\n",
    "positive_reviews[\"label\"] = 1\n",
    "ads_reviews[\"label\"] = 0\n",
    "indirect_reviews[\"label\"] = 0\n",
    "irelevant_reviews[\"label\"] = 0\n",
    "\n",
    "all_reviews = pd.concat([positive_reviews, ads_reviews, indirect_reviews, irelevant_reviews], ignore_index=True)\n",
    "all_reviews[\"context_vector\"] = all_reviews[\"gmap_id\"].map(vectors_map)\n",
    "all_reviews = all_reviews.dropna(subset=[\"context_vector\"]).reset_index(drop=True)\n",
    "\n",
    "gmap_id_labels = all_reviews.groupby('gmap_id')['label'].agg(lambda x: x.mode()[0])\n",
    "gmap_id_labels = gmap_id_labels.reset_index(name='majority_label')\n",
    "\n",
    "all_gmap_ids = gmap_id_labels['gmap_id'].values\n",
    "majority_labels = gmap_id_labels['majority_label'].values\n",
    "\n",
    "train_ids, temp_ids, _, temp_labels = train_test_split(\n",
    "    all_gmap_ids, majority_labels,\n",
    "    test_size=validation_ratio + test_ratio,\n",
    "    stratify=majority_labels,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "val_ids, test_ids, _, _ = train_test_split(\n",
    "    temp_ids, temp_labels,\n",
    "    test_size=test_ratio / (validation_ratio + test_ratio),\n",
    "    stratify=temp_labels,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "train_df = all_reviews[all_reviews[\"gmap_id\"].isin(train_ids)]\n",
    "val_df = all_reviews[all_reviews[\"gmap_id\"].isin(val_ids)]\n",
    "test_df = all_reviews[all_reviews[\"gmap_id\"].isin(test_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2336cf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Dataset Sizes ---\n",
      "Training set size: 38058\n",
      "Validation set size: 11610\n",
      "Test set size: 4969\n",
      "\n",
      "--- Final Class Distribution by Review (Correct) ---\n",
      "Training set distribution:\n",
      "label\n",
      "0    0.506569\n",
      "1    0.493431\n",
      "Name: proportion, dtype: float64\n",
      "Validation set distribution:\n",
      "label\n",
      "0    0.50267\n",
      "1    0.49733\n",
      "Name: proportion, dtype: float64\n",
      "Test set distribution:\n",
      "label\n",
      "0    0.543369\n",
      "1    0.456631\n",
      "Name: proportion, dtype: float64\n",
      "Congratulations! The gmap_id lists are mutually exclusive.\n",
      "Congratulations! All data leakage checks passed.\n",
      "Your datasets are now correctly partitioned by gmap_id.\n"
     ]
    }
   ],
   "source": [
    "# Verification prints\n",
    "print(\"--- Final Dataset Sizes ---\")\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "print(\"\\n--- Final Class Distribution by Review (Correct) ---\")\n",
    "print(f\"Training set distribution:\\n{train_df['label'].value_counts(normalize=True)}\")\n",
    "print(f\"Validation set distribution:\\n{val_df['label'].value_counts(normalize=True)}\")\n",
    "print(f\"Test set distribution:\\n{test_df['label'].value_counts(normalize=True)}\")\n",
    "\n",
    "train_ids_set = set(train_ids)\n",
    "val_ids_set = set(val_ids)\n",
    "test_ids_set = set(test_ids)\n",
    "\n",
    "# Check for overlaps between the ID sets\n",
    "assert len(train_ids_set.intersection(val_ids_set)) == 0, \"gmap_id leakage between training and validation sets!\"\n",
    "assert len(train_ids_set.intersection(test_ids_set)) == 0, \"gmap_id leakage between training and test sets!\"\n",
    "assert len(val_ids_set.intersection(test_ids_set)) == 0, \"gmap_id leakage between validation and test sets!\"\n",
    "\n",
    "print(\"Congratulations! The gmap_id lists are mutually exclusive.\")\n",
    "\n",
    "train_indices = set(train_df.index)\n",
    "val_indices = set(val_df.index)\n",
    "test_indices = set(test_df.index)\n",
    "\n",
    "# Check for overlaps between the sets\n",
    "assert len(train_indices.intersection(val_indices)) == 0, \"Data leakage found between training and validation sets!\"\n",
    "assert len(train_indices.intersection(test_indices)) == 0, \"Data leakage found between training and test sets!\"\n",
    "assert len(val_indices.intersection(test_indices)) == 0, \"Data leakage found between validation and test sets!\"\n",
    "\n",
    "print(\"Congratulations! All data leakage checks passed.\")\n",
    "print(\"Your datasets are now correctly partitioned by gmap_id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b86531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Consistency and Label Check ---\n",
      "Total number of reviews: 54637\n",
      "Number of 'positive' reviews (label 1): 26822\n",
      "Number of 'other' reviews (label 0): 27815\n",
      "\n",
      "--- Class Distribution Check ---\n",
      "Overall Label Distribution:\n",
      "label\n",
      "0    27815\n",
      "1    26822\n",
      "Name: count, dtype: int64\n",
      "Overall Label Ratio:\n",
      "label\n",
      "0    0.509087\n",
      "1    0.490913\n",
      "Name: proportion, dtype: float64\n",
      "All initial data consistency and label checks passed.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Data Consistency and Label Check ---\")\n",
    "print(f\"Total number of reviews: {len(all_reviews)}\")\n",
    "pos_count = all_reviews[all_reviews['label'] == 1].shape[0]\n",
    "neg_count = all_reviews[all_reviews['label'] == 0].shape[0]\n",
    "\n",
    "print(f\"Number of 'positive' reviews (label 1): {pos_count}\")\n",
    "print(f\"Number of 'other' reviews (label 0): {neg_count}\")\n",
    "assert pos_count == len(positive_reviews), \"Positive label count does not match original file size.\"\n",
    "assert neg_count == len(ads_reviews) + len(indirect_reviews) + len(irelevant_reviews), \"Negative label count does not match original files size.\"\n",
    "\n",
    "print(\"\\n--- Class Distribution Check ---\")\n",
    "label_counts = all_reviews['label'].value_counts()\n",
    "print(f\"Overall Label Distribution:\\n{label_counts}\")\n",
    "print(f\"Overall Label Ratio:\\n{all_reviews['label'].value_counts(normalize=True)}\")\n",
    "\n",
    "unexpected_labels = all_reviews['label'].isin([0, 1])\n",
    "if not unexpected_labels.all():\n",
    "    print(\"Warning: Found labels other than 0 or 1.\")\n",
    "\n",
    "print(\"All initial data consistency and label checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83365735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, contexts, inputs, targets):\n",
    "        assert len(contexts) == len(inputs) == len(targets)\n",
    "        self.contexts = contexts\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = torch.tensor(self.contexts[idx], dtype=torch.float)\n",
    "        inp = self.inputs[idx]\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return context, inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13a64d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "\n",
    "train_dataset = ReviewDataset(\n",
    "    contexts=train_df[\"context_vector\"].tolist(),\n",
    "    inputs=train_df[\"text\"].tolist(),\n",
    "    targets=train_df[\"label\"].tolist()\n",
    ")\n",
    "\n",
    "val_dataset = ReviewDataset(\n",
    "    contexts=val_df[\"context_vector\"].tolist(),\n",
    "    inputs=val_df[\"text\"].tolist(),\n",
    "    targets=val_df[\"label\"].tolist()\n",
    ")\n",
    "\n",
    "test_dataset = ReviewDataset(\n",
    "    contexts=test_df[\"context_vector\"].tolist(),\n",
    "    inputs=test_df[\"text\"].tolist(),\n",
    "    targets=test_df[\"label\"].tolist()\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a091b738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Leakage Check ---\n",
      "No data leakage detected in the train/val/test split.\n",
      "\n",
      "--- Split Ratio and Class Distribution Check ---\n",
      "Total dataset size: 54637\n",
      "Training set size: 38058 (69.66%)\n",
      "Validation set size: 11610 (21.25%)\n",
      "Test set size: 4969 (9.09%)\n",
      "\n",
      "Training set distribution:\n",
      " label\n",
      "0    0.506569\n",
      "1    0.493431\n",
      "Name: proportion, dtype: float64\n",
      "Validation set distribution:\n",
      " label\n",
      "0    0.50267\n",
      "1    0.49733\n",
      "Name: proportion, dtype: float64\n",
      "Test set distribution:\n",
      " label\n",
      "0    0.543369\n",
      "1    0.456631\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- DataLoader Sanity Check ---\n",
      "Batch loaded successfully from train_loader.\n",
      "Contexts batch shape: torch.Size([96, 768])\n",
      "Inputs batch (first 5): ('The seats are not too far up from the field.\\nI enjoyed the Suite seats.\\nI love this location than the old RFK Stadium.\\nRooney Rocks! #9', 'I read that the phone repair service offers a warranty on their repairs, giving customers peace of mind.', 'Awesome place to watch games', 'Well made, stylish and beneficial !  I made sure to pull out my Skatsz  lined hat for the anticipated cold weather last week. Nothing else will do! This is it PERIOD. Get you one to understand and you will over stand.', 'Reasonable selection at FreshMart. Friendly staff too.')\n",
      "Targets batch shape: torch.Size([96])\n",
      "Target values (first 5): [1.0, 0.0, 1.0, 1.0, 0.0]\n",
      "\n",
      "All DataLoader sanity checks passed.\n",
      "\n",
      "--- Checking Label Distribution for Training Set ---\n",
      "Total samples: 38058\n",
      "Positive labels (1): 18779 (49.34%)\n",
      "Negative labels (0): 19279 (50.66%)\n",
      "\n",
      "--- Checking Label Distribution for Validation Set ---\n",
      "Total samples: 11610\n",
      "Positive labels (1): 5774 (49.73%)\n",
      "Negative labels (0): 5836 (50.27%)\n",
      "\n",
      "--- Checking Label Distribution for Test Set ---\n",
      "Total samples: 4969\n",
      "Positive labels (1): 2269 (45.66%)\n",
      "Negative labels (0): 2700 (54.34%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Data Leakage Check ---\")\n",
    "# Use the index of the original DataFrame to check for overlaps\n",
    "train_indices = set(train_df.index)\n",
    "val_indices = set(val_df.index)\n",
    "test_indices = set(test_df.index)\n",
    "\n",
    "print(\"No data leakage detected in the train/val/test split.\")\n",
    "\n",
    "# --- Split Ratio and Class Distribution Check ---\n",
    "\n",
    "print(\"\\n--- Split Ratio and Class Distribution Check ---\")\n",
    "total_size = len(all_reviews)\n",
    "print(f\"Total dataset size: {total_size}\")\n",
    "print(f\"Training set size: {len(train_df)} ({len(train_df)/total_size:.2%})\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(val_df)/total_size:.2%})\")\n",
    "print(f\"Test set size: {len(test_df)} ({len(test_df)/total_size:.2%})\")\n",
    "\n",
    "print(\"\\nTraining set distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
    "print(\"Validation set distribution:\\n\", val_df['label'].value_counts(normalize=True))\n",
    "print(\"Test set distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n--- DataLoader Sanity Check ---\")\n",
    "\n",
    "# Inspect a single batch from the training loader\n",
    "try:\n",
    "    context_batch, input_batch, target_batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"Batch loaded successfully from train_loader.\")\n",
    "    print(f\"Contexts batch shape: {context_batch.shape}\")\n",
    "    print(f\"Inputs batch (first 5): {input_batch[:5]}\")\n",
    "    print(f\"Targets batch shape: {target_batch.shape}\")\n",
    "    print(f\"Target values (first 5): {target_batch[:5].tolist()}\")\n",
    "    \n",
    "    assert context_batch.shape[0] == len(input_batch) == target_batch.shape[0], \"Batch size mismatch.\"\n",
    "    assert context_batch.dtype == torch.float, \"Context tensor has incorrect dtype.\"\n",
    "    assert target_batch.dtype == torch.float, \"Target tensor has incorrect dtype.\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading a batch from train_loader: {e}\")\n",
    "\n",
    "# You can do the same for val_loader and test_loader if you wish.\n",
    "print(\"\\nAll DataLoader sanity checks passed.\")\n",
    "\n",
    "def get_label_distribution(data_loader: DataLoader, dataset_name: str):\n",
    "    \"\"\"\n",
    "    Iterates through a DataLoader and calculates the distribution of positive and negative labels.\n",
    "\n",
    "    Args:\n",
    "        data_loader (DataLoader): The DataLoader object to iterate over.\n",
    "        dataset_name (str): The name of the dataset (e.g., 'Training', 'Validation').\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Checking Label Distribution for {dataset_name} Set ---\")\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Ensure the loader is not empty before starting\n",
    "    if len(data_loader.dataset) == 0:\n",
    "        print(f\"The {dataset_name} dataset is empty.\")\n",
    "        return\n",
    "\n",
    "    # Use no_grad to improve efficiency, as we are only reading data\n",
    "    with torch.no_grad():\n",
    "        for _, _, targets in data_loader:\n",
    "            targets = targets.cpu().numpy()\n",
    "            \n",
    "            positive_count += (targets == 1).sum()\n",
    "            negative_count += (targets == 0).sum()\n",
    "            total_count += targets.shape[0]\n",
    "\n",
    "    # Verify that the total count matches the dataset size\n",
    "    assert total_count == len(data_loader.dataset), \\\n",
    "           f\"Count mismatch! {dataset_name} counted {total_count} samples, but dataset size is {len(data_loader.dataset)}\"\n",
    "\n",
    "    # Calculate percentages\n",
    "    if total_count > 0:\n",
    "        positive_percentage = (positive_count / total_count) * 100\n",
    "        negative_percentage = (negative_count / total_count) * 100\n",
    "    else:\n",
    "        positive_percentage = 0\n",
    "        negative_percentage = 0\n",
    "        \n",
    "    print(f\"Total samples: {total_count}\")\n",
    "    print(f\"Positive labels (1): {positive_count} ({positive_percentage:.2f}%)\")\n",
    "    print(f\"Negative labels (0): {negative_count} ({negative_percentage:.2f}%)\")\n",
    "\n",
    "\n",
    "# Now, call the function for each of your DataLoaders\n",
    "# NOTE: You must run this AFTER the code that defines train_loader, val_loader, and test_loader.\n",
    "# If those objects are not yet defined, this script will fail.\n",
    "\n",
    "get_label_distribution(train_loader, \"Training\")\n",
    "get_label_distribution(val_loader, \"Validation\")\n",
    "get_label_distribution(test_loader, \"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8feb1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_training(\n",
    "    embedder,\n",
    "    decoder: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    num_epochs: int,\n",
    "    checkpoint_interval: int, \n",
    "    path: str,\n",
    "    continue_checkpoint: bool = False,\n",
    "    writer: torch.utils.tensorboard.SummaryWriter | None = None,\n",
    "    device: torch.device = None,\n",
    "):\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    embedder.to(device)\n",
    "    decoder.to(device)\n",
    "    decoder.train()\n",
    "\n",
    "    if not continue_checkpoint:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        run_name = f\"{decoder.__class__.__name__}_{timestamp}\"\n",
    "        log_dir = os.path.join(path, run_name)\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        start_epoch = 1\n",
    "    else:\n",
    "        all_ckpts = glob.glob(os.path.join(path, f\"{decoder.__class__.__name__}_*\", \"checkpoint_epoch_*.pt\"))\n",
    "        if not all_ckpts:\n",
    "            raise FileNotFoundError(\"No checkpoint found to continue from.\")\n",
    "        last_ckpt = max(all_ckpts, key=os.path.getctime)\n",
    "        log_dir = os.path.dirname(last_ckpt)\n",
    "        checkpoint = torch.load(last_ckpt, map_location=device)\n",
    "        decoder.load_state_dict(checkpoint[\"decoder_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Resuming from checkpoint {last_ckpt}, starting at epoch {start_epoch}\")\n",
    "\n",
    "    if writer is None:\n",
    "        writer_dir = os.path.join(log_dir, \"writer\")\n",
    "        os.makedirs(writer_dir, exist_ok=True)\n",
    "        writer = SummaryWriter(log_dir=writer_dir)\n",
    "\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=num_epochs,\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "    for _ in range(start_epoch - 1):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        all_targets = []\n",
    "        all_preds = []\n",
    "        num_batches = len(train_loader)\n",
    "\n",
    "        for batch_idx, (contexts_embed, inputs, targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch}\")):\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            contexts_embed = contexts_embed.to(device)\n",
    "            # STUDY ON HOW CONTEXT AFFECTS ACCURACY\n",
    "            # batch_size = targets.shape[0]\n",
    "            # contexts_embed = torch.zeros(batch_size, 768, device=device)\n",
    "\n",
    "            inputs_embed = embedder.embed(inputs, return_tokens=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = decoder(inputs_embed, contexts_embed).squeeze(-1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            global_step = (epoch - 1) * num_batches + batch_idx\n",
    "            writer.add_scalar('Train/Batch_Loss', loss.item(), global_step)\n",
    "\n",
    "            all_targets.append(targets.detach().cpu())\n",
    "            all_preds.append(outputs.detach().cpu())\n",
    "\n",
    "            del inputs_embed, outputs, targets, contexts_embed\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        writer.add_scalar(\"Train/LR\", current_lr, epoch)\n",
    "\n",
    "        all_targets = torch.cat(all_targets).numpy()\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        pred_labels = (all_preds >= 0.5).astype(int)\n",
    "\n",
    "        avg_loss = running_loss / num_batches\n",
    "        accuracy = (pred_labels == all_targets).mean()\n",
    "        f1 = f1_score(all_targets, pred_labels)\n",
    "\n",
    "        writer.add_scalar(\"Train/Epoch_Avg_Loss\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Train/Accuracy\", accuracy, epoch)\n",
    "        writer.add_scalar(\"Train/F1\", f1, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch} - Loss: {avg_loss:.4f}, Acc: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        decoder.eval()\n",
    "        val_targets, val_preds = [], []\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for contexts_embed, inputs, targets in tqdm(val_loader, desc=f\"Epoch {epoch} - Validation\"):\n",
    "                targets = targets.float().to(device)\n",
    "                contexts_embed = contexts_embed.to(device)\n",
    "                inputs_embed = embedder.embed(inputs, return_tokens=True)\n",
    "\n",
    "                outputs = decoder(inputs_embed, contexts_embed).squeeze(-1)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                val_targets.append(targets.detach().cpu())\n",
    "                val_preds.append(outputs.detach().cpu())\n",
    "\n",
    "                del inputs_embed, outputs, targets, contexts_embed\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        val_targets = torch.cat(val_targets).numpy()\n",
    "        val_preds = torch.cat(val_preds).numpy()\n",
    "        val_pred_labels = (val_preds >= 0.5).astype(int)\n",
    "\n",
    "        val_avg_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = (val_pred_labels == val_targets).mean()\n",
    "        val_f1 = f1_score(val_targets, val_pred_labels)\n",
    "\n",
    "        writer.add_scalar(\"Val/Loss\", val_avg_loss, epoch)\n",
    "        writer.add_scalar(\"Val/Accuracy\", val_accuracy, epoch)\n",
    "        writer.add_scalar(\"Val/F1\", val_f1, epoch)\n",
    "\n",
    "        print(f\"Validation - Loss: {val_avg_loss:.4f}, Acc: {val_accuracy:.4f}, F1: {val_f1:.4f}\")\n",
    "        decoder.train()\n",
    "\n",
    "        if (epoch % checkpoint_interval == 0) or (epoch == start_epoch):\n",
    "            checkpoint_file = os.path.join(log_dir, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"decoder_state_dict\": decoder.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": avg_loss\n",
    "            }, checkpoint_file)\n",
    "            print(f\"Saved checkpoint: {checkpoint_file}\")\n",
    "\n",
    "    return avg_loss, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e87625d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "d_input = 768\n",
    "d_context = 768\n",
    "d_model = 256\n",
    "num_layers = 3\n",
    "\n",
    "mamba_pooled = MambaPooled(\n",
    "    num_layers=num_layers,\n",
    "    d_input=d_input,\n",
    "    d_model=d_model,\n",
    "    d_context=d_context,\n",
    "    d_state=16,\n",
    "    d_discr=None,\n",
    "    ker_size=4,\n",
    "    parallel=False\n",
    ")\n",
    "embedder = NomicEmbedder()\n",
    "optimizer = torch.optim.AdamW(mamba_pooled.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a873e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 397/397 [03:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.0795, Acc: 0.9704, F1: 0.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation: 100%|██████████| 121/121 [00:33<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1605, Acc: 0.9394, F1: 0.9415\n",
      "Saved checkpoint: C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\weights\\mamba_pooled\\MambaPooled_20250827_221728\\checkpoint_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 397/397 [04:02<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.0178, Acc: 0.9938, F1: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation: 100%|██████████| 121/121 [00:35<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1043, Acc: 0.9609, F1: 0.9613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 397/397 [04:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.0089, Acc: 0.9971, F1: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1610, Acc: 0.9436, F1: 0.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 397/397 [03:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 0.0062, Acc: 0.9978, F1: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1256, Acc: 0.9663, F1: 0.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 397/397 [03:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 0.0074, Acc: 0.9974, F1: 0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1484, Acc: 0.9568, F1: 0.9554\n",
      "Saved checkpoint: C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\weights\\mamba_pooled\\MambaPooled_20250827_221728\\checkpoint_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 397/397 [03:58<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 0.0055, Acc: 0.9982, F1: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1013, Acc: 0.9710, F1: 0.9714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 397/397 [03:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 0.0037, Acc: 0.9987, F1: 0.9987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Validation: 100%|██████████| 121/121 [00:33<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1037, Acc: 0.9720, F1: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 397/397 [04:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 0.0031, Acc: 0.9989, F1: 0.9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0886, Acc: 0.9746, F1: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 397/397 [04:04<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 0.0023, Acc: 0.9994, F1: 0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1171, Acc: 0.9764, F1: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 397/397 [03:58<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Loss: 0.0047, Acc: 0.9984, F1: 0.9984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Validation: 100%|██████████| 121/121 [00:33<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0982, Acc: 0.9728, F1: 0.9729\n",
      "Saved checkpoint: C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\weights\\mamba_pooled\\MambaPooled_20250827_221728\\checkpoint_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 397/397 [04:12<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Loss: 0.0024, Acc: 0.9993, F1: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Validation: 100%|██████████| 121/121 [00:35<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1117, Acc: 0.9740, F1: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 397/397 [04:09<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Loss: 0.0041, Acc: 0.9988, F1: 0.9988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Validation: 100%|██████████| 121/121 [00:35<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1974, Acc: 0.9237, F1: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 397/397 [04:08<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Loss: 0.0042, Acc: 0.9986, F1: 0.9986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Validation: 100%|██████████| 121/121 [00:36<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0655, Acc: 0.9829, F1: 0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 397/397 [04:03<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Loss: 0.0012, Acc: 0.9997, F1: 0.9997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Validation: 100%|██████████| 121/121 [00:33<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0831, Acc: 0.9809, F1: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 397/397 [04:01<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Loss: 0.0025, Acc: 0.9992, F1: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.0834, Acc: 0.9798, F1: 0.9798\n",
      "Saved checkpoint: C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\weights\\mamba_pooled\\MambaPooled_20250827_221728\\checkpoint_epoch_15.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 397/397 [04:01<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Loss: 0.0001, Acc: 1.0000, F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Validation: 100%|██████████| 121/121 [00:33<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1324, Acc: 0.9805, F1: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 397/397 [03:59<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Validation: 100%|██████████| 121/121 [00:33<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1369, Acc: 0.9803, F1: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 397/397 [03:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1494, Acc: 0.9804, F1: 0.9805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 397/397 [04:01<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1509, Acc: 0.9813, F1: 0.9813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 397/397 [04:01<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Loss: 0.0000, Acc: 1.0000, F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Validation: 100%|██████████| 121/121 [00:34<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Loss: 0.1637, Acc: 0.9811, F1: 0.9812\n",
      "Saved checkpoint: C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\weights\\mamba_pooled\\MambaPooled_20250827_221728\\checkpoint_epoch_20.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21:  18%|█▊        | 70/397 [00:41<03:12,  1.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mclassification_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmamba_pooled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mian\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCoding\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mReviewClassification\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmamba_pooled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinue_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 72\u001b[0m, in \u001b[0;36mclassification_training\u001b[1;34m(embedder, decoder, optimizer, train_loader, val_loader, num_epochs, checkpoint_interval, path, continue_checkpoint, writer, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m outputs \u001b[38;5;241m=\u001b[39m decoder(inputs_embed, contexts_embed)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m---> 72\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     75\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classification_training(\n",
    "    embedder,\n",
    "    decoder=mamba_pooled,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=100,\n",
    "    checkpoint_interval=5, \n",
    "    path=r\"C:\\Users\\ian\\Desktop\\Coding\\ReviewClassification\\model\\weights\\mamba_pooled\",\n",
    "    continue_checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380b7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 1])\n",
      "Embeddings shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_input = 512\n",
    "d_context = 512\n",
    "\n",
    "# Random inputs\n",
    "x = torch.randn(batch_size, seq_len, d_input)      # input sequence\n",
    "context = torch.randn(batch_size, d_context)      # context vector\n",
    "\n",
    "# Forward pass\n",
    "logits = mamba_pooled(x, context)                # classifier output\n",
    "embeddings = mamba_pooled(x, context, True)      # embeddings\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)             # (batch_size, seq_len, d_model)\n",
    "print(\"Embeddings shape:\", embeddings.shape)     # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc48d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_input = 512        \n",
    "d_context = 512      \n",
    "d_model = 256        \n",
    "num_layers = 3       \n",
    "\n",
    "mamba_cls = MambaCLS(\n",
    "    num_layers=num_layers,\n",
    "    d_input=d_input,\n",
    "    d_context=d_context,\n",
    "    d_model=d_model,\n",
    "    d_state=16,       \n",
    "    d_discr=None,     \n",
    "    ker_size=4,       \n",
    "    parallel=True    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db202b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 1])\n",
      "Embeddings shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_input = 512\n",
    "d_context = 512\n",
    "\n",
    "# Random inputs\n",
    "x = torch.randn(batch_size, seq_len, d_input)      # input sequence\n",
    "context = torch.randn(batch_size, d_context)      # context vector\n",
    "\n",
    "# Forward pass\n",
    "logits = mamba_cls(x, context)                # classifier output\n",
    "embeddings = mamba_cls(x, context, True)      # embeddings\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)             # (batch_size, seq_len, d_model)\n",
    "print(\"Embeddings shape:\", embeddings.shape)     # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5402ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_input = 512\n",
    "d_context = 512\n",
    "d_model = 256\n",
    "d_layers = 3       \n",
    "d_heads = 8 \n",
    "dropout = 0.1\n",
    "\n",
    "cross_attn_model = CrossAttentionTransformer(\n",
    "    d_input=d_input,\n",
    "    d_context=d_context,\n",
    "    d_model=d_model,\n",
    "    d_layers=d_layers,\n",
    "    d_heads=d_heads,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bdc09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 10, 1])\n",
      "Embeddings shape: torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_input = 512\n",
    "d_context = 512\n",
    "\n",
    "# Random inputs\n",
    "x = torch.randn(batch_size, seq_len, d_input)      # input sequence\n",
    "context = torch.randn(batch_size, d_context)      # context vector\n",
    "\n",
    "# Forward pass\n",
    "logits = cross_attn_model(x, context)                # classifier output\n",
    "embeddings = cross_attn_model(x, context, True)      # embeddings\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)             # (batch_size, seq_len, d_model)\n",
    "print(\"Embeddings shape:\", embeddings.shape)     # (batch_size, seq_len, d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
